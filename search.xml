<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>debate</title>
    <url>/posts/ddc4a368/</url>
    <content><![CDATA[<blockquote class="blockquote-center"><p>Some debate topics in English. Practice oral English every day.</p>
</blockquote>
<a id="more"></a>
<h1 id="Can-people-work-from-home"><a href="#Can-people-work-from-home" class="headerlink" title="Can people work from home?"></a>Can people work from home?</h1><h2 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h2><h2 id="Disadvantages"><a href="#Disadvantages" class="headerlink" title="Disadvantages"></a>Disadvantages</h2><ol>
<li>different industry, actor , hotel stores ,designer need to collect some info with people. some work like recruiting need to face to face commucation. Most of the time, they need to haave a look. TO keep the staff working ,boss need to keep an eye on the employees.For the majority of company, even people are statying in the workplace, the y make mistakes.</li>
<li>psychology idea, people and human being need to social, they can create new and ideas when they face to face, build a relationshiop with customers, but phone cannot. observing their body language, gestures, eye contact. Face to face talk is easy to show your facial emotions, they can ask questions.</li>
<li>senior manager, they need to have a meeting, decisive decision? show their power,if this is very important and interesting for you, you need to find it, if it’s wrong,you need to make sure everything is under control. contract is face to face, if they have a contract online, it&#39;s not reliable.</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>IELTS</title>
    <url>/posts/6a3e8990/</url>
    <content><![CDATA[<blockquote class="blockquote-center"><p>The latest IELTS topics in part2 and part3. Practice oral English every day.</p>
</blockquote>
<a id="more"></a>
<h1 id="IELTS-CUE-CARD-–-Talk-about-a-person-who-taught-you-something-important"><a href="#IELTS-CUE-CARD-–-Talk-about-a-person-who-taught-you-something-important" class="headerlink" title="IELTS CUE CARD – Talk about a person who taught you something important"></a>IELTS CUE CARD – Talk about a person who taught you something important</h1><ul>
<li>Who the person is?</li>
<li>What he/she is like?</li>
<li>What he/she taught you?</li>
<li>How you felt about him/ her?</li>
</ul>
<p>I’d like to talk about my mother, from whom I’ve learned considerable knowledge. Out of diverse skills she taught me, I would focus on cooking. She is an excellent cook and I, of course, is lucky to enjoy different tasty dishes each day. Ever since I made a plan to go abroad for my higher education, my mother started involving me in the kitchen with great patience, for I should look after myself without her being around.</p>
<p>Knowing that I had no idea of cooking and foods, she asked me to accompany her to the supermarket to get familiarized with various food. Then I was put in charge of washing all the stuff we bought for cooking and was made to stand beside her when she cooked, showing me how to do things. After a week’s time, she left me to have a try, though I felt I was not ready. Encouraged and instructed by her, I made my first dish, although it was not as delicious as that made by my mother. Anyhow, I made it. My mother was much happier than I did, saying that I would surely be a good cook like her.</p>
<p>Busy with my work, I still cook when I am on a day off. Sometimes, I will try a different way to cook, and I’d like to share my experience with mother about cooking, which she is eager to listen to and she is happy to try a new way, too. Learning to cook makes my life much easier and independent, and it is inevitable that I’ll be able to use it for the rest of my life. So I feel grateful to my mother.</p>
<p>Follow Ups:</p>
<p>Describe one of your family members you spend most of the time with?</p>
<p>who it is<br>when you are usually together<br>what you do together<br>and explain why you spend the most time with this member of your family</p>
<p>I have always been interested in spending time with superior people because, in this way, I could learn new things from them and learn approaches to survive in this challenging world. I’d like to talk about one family member with whom I want to spend my quality time.</p>
<p>He is my grandfather. My grandfather is in his 70s. But he is still energetic and lively. He likes to do some outdoor activities, such as fishing and strolling.</p>
<p>He is a very extroverted and talktive person. He is a knowledgeable person, so I always take help in my studies. He was a history teacher before so he enjoys to tell the stories to others. My Grandfather gave excellent knowledge about our earlier history.</p>
<p>There are many reasons why I like to spend time with him. First of all, he always encourages me to face any challenge in my life. For example,<br>In addition, I like to share knowledge with him because he has excellent experience with their life; hence he always gives me the best advice and wishes to take notice from him.</p>
<p>Furthermore, He always supports me in every work, and he teaches me what thing is right or wrong for me. Apart from it, He has a whimsical nature, so sometimes he makes some fun. He gives me information about our history. Last but not least,</p>
<p>Reference:</p>
<ol>
<li><a href="https://ieltsfever.org/person-who-taught-you-a-useful-skill/" target="_blank" rel="noopener">IELTS Fever</a></li>
</ol>
]]></content>
      <categories>
        <category>English study</category>
      </categories>
      <tags>
        <tag>English study</tag>
      </tags>
  </entry>
  <entry>
    <title>big data revision</title>
    <url>/posts/1984119a/</url>
    <content><![CDATA[<blockquote class="blockquote-center"><p>A revision for Big Data System.</p>
</blockquote>
<a id="more"></a>
<h1 id="Big-Data-Definitions"><a href="#Big-Data-Definitions" class="headerlink" title="Big Data Definitions"></a>Big Data Definitions</h1><ul>
<li>huge volumes of data</li>
<li>the challenges, risks, and rewards of storing and extracting meaningful information from this data</li>
<li>it becomes difficult to process using on-hand data management tools or traditional data processing applications<h2 id="Volume"><a href="#Volume" class="headerlink" title="Volume"></a>Volume</h2></li>
<li>Sheer size of data in terms of storage and access.</li>
<li>Many different factors can contribute to the increase in data volume.<h2 id="Velocity"><a href="#Velocity" class="headerlink" title="Velocity"></a>Velocity</h2></li>
<li>Speed of incoming data and the time it takes to process.</li>
<li>Data velocity is both the speed at which data streams in, and the timely manner in which data must be dealt with to maintain time based relevance.<h2 id="Variety"><a href="#Variety" class="headerlink" title="Variety"></a>Variety</h2></li>
<li>Types of files and formats of data as well as sources.</li>
<li>Data comes in all kinds of formats, but can be grouped into two types: structured and unstructured.</li>
<li>Structured data is the numeric data in traditional databases. Created from line-of-business and pre-formatted data collected over time.</li>
<li>Unstructured data is the relational and seemingly unrelated data that comes from unstructured sources (social media, text documents, email, video, audio, etc.)</li>
</ul>
<blockquote>
<p>Extra Vs have been proposed, these typically describe characteristics rather than being definitional</p>
</blockquote>
<h2 id="Veracity"><a href="#Veracity" class="headerlink" title="Veracity"></a>Veracity</h2><ul>
<li>How accurate and meaningful is the data?<h2 id="Value"><a href="#Value" class="headerlink" title="Value"></a>Value</h2></li>
<li>How can value be derived from a big dataset?</li>
</ul>
<h1 id="Big-Data-Examples"><a href="#Big-Data-Examples" class="headerlink" title="Big Data Examples"></a>Big Data Examples</h1><ul>
<li>smart healthcare</li>
<li>wearable devices</li>
<li>smart retail</li>
<li>equipment health management</li>
<li>connected/smart cars</li>
<li>intelligent transport</li>
<li>smart grid</li>
<li>smart cities</li>
<li>recommender systems</li>
<li>finance</li>
</ul>
<h1 id="HIGH-LEVEL-ARCHITECTURE"><a href="#HIGH-LEVEL-ARCHITECTURE" class="headerlink" title="HIGH LEVEL ARCHITECTURE"></a>HIGH LEVEL ARCHITECTURE</h1><ul>
<li>The challenge of Big Data is how can we possibly deal with data that fits    these characteristics, both individually and collectively.</li>
<li>The core of the answer lies in using distributed resources.<h2 id="Big-Data-sources"><a href="#Big-Data-sources" class="headerlink" title="Big Data sources"></a>Big Data sources</h2></li>
<li>The    sources    layer    refers to all    of the data available for analysis, coming in from all channels.    The    type of analysis and sources are closely related.<h2 id="Data-messaging-and-store-layer"><a href="#Data-messaging-and-store-layer" class="headerlink" title="Data messaging and store layer"></a>Data messaging and store layer</h2></li>
<li>is responsible for acquiring data from data sources.</li>
<li>If    necessary,    the    layer    will also be responsible    for    converting    the    data    to    a format    that    suits    how    the    data will    be analysed.</li>
<li>Data acquisition</li>
<li>Data digest</li>
<li>Distributed file storage<h2 id="Analysis-layer"><a href="#Analysis-layer" class="headerlink" title="Analysis layer"></a>Analysis layer</h2></li>
<li>The    analysis layer reads the data digested by the data massaging    and    store    layer    (in    some    cases, it accesses data directly from the data source).</li>
<li>Entity indentification</li>
<li>Analytics engine</li>
<li>Model management<h2 id="CONSUMPTION-LAYER"><a href="#CONSUMPTION-LAYER" class="headerlink" title="CONSUMPTION LAYER"></a>CONSUMPTION LAYER</h2></li>
<li>This    layer    consumes    the    output    provided    by    the    analysis    layer.<h2 id="INFORMATION-INTEGRATION-LAYER"><a href="#INFORMATION-INTEGRATION-LAYER" class="headerlink" title="INFORMATION    INTEGRATION    LAYER"></a>INFORMATION    INTEGRATION    LAYER</h2></li>
<li>Responsible    for    connecting    to    various    data    sources.</li>
<li>Requires    quality    connectors &amp;    adapters<h2 id="DATA-GOVERNANCE-LAYER"><a href="#DATA-GOVERNANCE-LAYER" class="headerlink" title="DATA    GOVERNANCE    LAYER"></a>DATA    GOVERNANCE    LAYER</h2></li>
<li>GDPR:    General    Data    Protection    Regulation<h2 id="SYSTEM-MANAGEMENT-layer"><a href="#SYSTEM-MANAGEMENT-layer" class="headerlink" title="SYSTEM    MANAGEMENT    layer"></a>SYSTEM    MANAGEMENT    layer</h2><h2 id="QUALITY-OF-SERVICE-QOS-LAYER"><a href="#QUALITY-OF-SERVICE-QOS-LAYER" class="headerlink" title="QUALITY OF    SERVICE    (QOS)    LAYER"></a>QUALITY OF    SERVICE    (QOS)    LAYER</h2></li>
</ul>
<h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><ul>
<li>Programming    model    for    expressing    distributed    computation    in massive-scale    systems.</li>
<li>Open-source    implementation    called    Hadoop.<h2 id="MapReduce-process"><a href="#MapReduce-process" class="headerlink" title="MapReduce    process"></a>MapReduce    process</h2></li>
<li>Iterate    over a large number of records.<br>‒ Extract    something    of interest from    each.(Map)<br>‒ Shuffle    and    sort intermediate    results.<br>‒ Aggregate    intermediate results into something usable.(Reduce)<br>‒ Generate final output.(Reduce)<h2 id="Distributed-File-System-DFS"><a href="#Distributed-File-System-DFS" class="headerlink" title="Distributed    File    System    (DFS)"></a>Distributed    File    System    (DFS)</h2></li>
<li>move    workers    to    the    data:<ul>
<li>Not    enough    RAM    to    hold    all    the    data    in    memory</li>
<li>Disk access is slow,    but    disk throughput is reasonable</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
      </tags>
  </entry>
  <entry>
    <title>papers summary</title>
    <url>/posts/7cd80329/</url>
    <content><![CDATA[<blockquote class="blockquote-center"><p>A summary of the papers that I read for the project.</p>
</blockquote>
<a id="more"></a>
<div class="note danger no-icon">
            <p>Please provide a brief summary including</p><ol><li>What did they do and why?</li><li>What did they find?</li><li>Are there any issues/challenges with the research?</li><li>How is their work relevant to your project?</li></ol>
          </div>

<div class="note info no-icon">
            <p>N. Valakunde and S. Ravikumar, “Prediction of Addiction to Social Media,” 2019 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT), Coimbatore, India, 2019, pp. 1-6.</p><ol><li><p>The problem of social media addiction has only been observed and studied in the previous research papers, but no solution has been presented to date. This study aims to solve the problem of addiction by implementing machine learning using Linear Regression through Least Square Method and recommending certain positive exercises and practices which can help an individual to refrain from high usage of social media.</p></li><li><p>The data collected is analyzed with the help of graphs plotted by using the users’ data. The inputs are taken from the user from the respective questions asked to him/her, and a point is plotted for the user’s values in the respective graphs. By plotting the points on these graphs and comparing them, the machine is able to determine the problem faced by an individual and help him/her by giving out suggestions in the form of helpline numbers, motivational videos and/or inspiring speeches from successful people. This is a small and effective method to help people get rid of addiction to social media and lead a better life.</p></li><li><p>The research uses a very simple and basic algorithm(Linear Regression using Least Square Method). More complex and advanced machine learning algorithms can be used to achieve higher levels of accuracy in predictions. Besides, the research only contains the user data collected of 287 users, more data should be collected to make result become much more intelligent and accurate.</p></li><li><p>The research uses Linear Regression algorithm to indicate whether the user is addicted to social media or not and gives some recommendation to them like cutting down on their social media usage. This work can be used to predict the user behaviors to indicate how much the user would get addicted to the social media.</p></li></ol>
          </div>

<div class="note warning no-icon">
            <p>M. Meghawat, S. Yadav, D. Mahata, Y. Yin, R. R. Shah and R. Zimmermann, “A Multimodal Approach to Predict Social Media Popularity,” 2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR), Miami, FL, 2018, pp. 190-195.</p><ol><li><p>First, they use a random forest approach on available social and contextual information. Next, they apply a CNN model on available social and contextual information. Moreover, they apply transfer learning on image content information using the pre-trained InceptionResnet V2 model. Finally, in their multimodal approach, they combine all features derived from earlier steps and apply a convolutional model. The first reason is that most of previous researches just focused on information provided in the SMP-T1 dataset which includes average counts for views, comments, tags, groups, members, and lengths of title and description. No such multimodal dataset exists for the prediction of social media photos. So they propose a new dataset, called multimodal-SMP-dataset, by augmenting the existing SMP-T1 dataset with additional contextual information such as titles, descriptions, and tags of social media photos in addition to crawling image content (actual photos). The second reason is that one modality might not be enough to solve a complex problem like popularity prediction, so their proposed system exploits multimodal information.</p></li><li><p>Experimental results confirm that despite they use half of the training set, their multimodal approach gives comparable performance to that of state-of-the-art. Moreover, they have provided a multimodal dataset for social media popularity prediction to research community.</p></li><li><p>In their research, they explored different models for social media popularity prediction and got different results (Spearman RHO, Mean Absolute Error (MAE), and Mean Squared Error (MSE)) for each model. The issue is that they didn’t mention how many times they did those experiments for each model. I do not know whether they used an average result of several experiments or just an optimal result of single experiment.</p></li><li><p>This research uses a multimodal approach to predict the social media popularity, which could help me consider to use a multimodal approach (including image content) to do my project instead of focusing on a single model. Besides, it reminds me that I could also expand the dataset if necessary.</p></li></ol>
          </div>

<div class="note success no-icon">
            
          </div>
]]></content>
      <categories>
        <category>papers</category>
      </categories>
      <tags>
        <tag>papers</tag>
      </tags>
  </entry>
  <entry>
    <title>COVID-19</title>
    <url>/posts/42fc5b46/</url>
    <content><![CDATA[<blockquote>
<p>最近看COVID-19的疫情统计，发现WHO的统计数据总是落后于各地政府的数据</p>
</blockquote>
<p>WHO(World Health Organization 世界卫生组织) 从1月21号开始，每天于欧洲中部时间(Central European Time)24H以内的数据，迄今为止，已经发布了共计66个每日报告了。<br>Data as reported by national authorities by 10:00 CET<br>但即使对于同一天的统计数据，依旧比政府的统计数据落后不少，原因如下：</p>
<p>Due to differences in reporting methods, retrospective data consolidation, and reporting delays, the number of new cases may not always reflect the exact difference between yesterday’s and today’s totals. WHO COVID-19 Situation Reports present official counts of confirmed COVID-19 cases, thus differences between WHO reports and other sources of COVID-19 data using different inclusion criteria and different data cutoff times are to be expected.</p>
]]></content>
      <categories>
        <category>Others</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title>Inspired</title>
    <url>/posts/bf1390a3/</url>
    <content><![CDATA[<blockquote>
<p>An email to record the concerns that I have recently. And thanks Professor Netta.</p>
</blockquote>
<a id="more"></a>
<p>Dear Netta,</p>
<p>I’m still at leeds now,  in residence during this time. I don’t have any plan to go back home in the near future.</p>
<p>– Ok, thanks!</p>
<p>When I went shopping, I found only myself wearing the mask, which made me feel a little uncomfortable. Some of my Chinese friends suffered from discrimination maybe because they are Chinese and they wear the masks.</p>
<p>– I also wore a mask yesterday and was also the only one in the shop and also got some funny looks. I think in general, we should expect a range of reactions. Some people think this is all crazy overreaction, others are down right panicking. If you experience any discrimination, please do let me know! Regardless, please don’t put yourself at risk just because of other people’s reactions.</p>
<p>I know self-control is pretty important, but when I study at home, I’m not very efficient. I tried to focus on the subjects that I’m interest in, maybe that’s one of the solutions.</p>
<p>–Jerry, this is not going to be easy. Remember that we are planning on a marathon, not a sprint. I would not expect you to work full hours initially. You need to set yourself up and gradually create an environment and routine that work for you. Go easy on yourself. One step at a time…</p>
<p>– take care! Netta</p>
<p>All the best to you and yours,<br>Jerry</p>
]]></content>
      <categories>
        <category>Others</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title>Tableau</title>
    <url>/posts/9c9742d/</url>
    <content><![CDATA[<blockquote>
<p>Just reord some issues when I use Tableau.</p>
</blockquote>
<a id="more"></a>
<ol>
<li>Create a Top N Filter顺序问题<br>From the Data pane, drag City to the Filters shelf：filter的City还是原数据里的所有City，不是通过filter State之后的City(top N filter or the set filter排序问题)<br>原因：Tableau Order of Operations(the order that Tableau performs various actions, such as the order in which it applies your filters to the view.)</li>
</ol>
<p>解决方法：On the Filters shelf, right-click the Inclusions (Country, State) (Country, State) set and select Add to Context.</p>
<p>Reference:</p>
<ol>
<li><a href="https://help.tableau.com/current/guides/get-started-tutorial/en-us/get-started-tutorial-drilldown.htm" target="_blank" rel="noopener">help.tableau.com</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>Heuristic Algorithm(启发式算法)</title>
    <url>/posts/f7ee5063/</url>
    <content><![CDATA[<h1 id="Heuristic-Algorithm-启发式算法"><a href="#Heuristic-Algorithm-启发式算法" class="headerlink" title="Heuristic Algorithm(启发式算法)"></a>Heuristic Algorithm(启发式算法)</h1><h2 id="general-concept"><a href="#general-concept" class="headerlink" title="general concept"></a>general concept</h2><p>a heuristic is a technique designed for solving a problem more quickly when classic methods are too slow, or for finding an approximate solution when classic methods fail to find any exact solution. This is achieved by trading optimality, completeness, accuracy, or precision for speed. In a way, it can be considered a shortcut.</p>
<a id="more"></a>
<h2 id="Travelling-salesman-problem-TSP"><a href="#Travelling-salesman-problem-TSP" class="headerlink" title="Travelling salesman problem(TSP)"></a>Travelling salesman problem(TSP)</h2><blockquote>
<p>Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city and returns to the origin city?</p>
</blockquote>
<p>TSP is known to be NP-Hard so an optimal solution for even a moderate size problem is difficult to solve. Instead, the greedy algorithm can be used to give a good but not optimal solution (it is an approximation to the optimal answer) in a reasonably short amount of time. The greedy algorithm heuristic says to pick whatever is currently the best next step regardless of whether that prevents (or even makes impossible) good steps later. It is a heuristic in that practice says it is a good enough solution, theory says there are better solutions (and even can tell how much better in some cases).</p>
<h2 id="Search"><a href="#Search" class="headerlink" title="Search"></a>Search</h2><p>Another example of heuristic making an algorithm faster occurs in certain search problems. Initially, the heuristic tries every possibility at each step, like the full-space search algorithm. But it can stop the search at any time if the current possibility is already worse than the best solution already found. In such search problems, a heuristic can be used to try good choices first so that bad paths can be eliminated early (see alpha-beta pruning). In the case of best-first search algorithms, such as A* search, the heuristic improves the algorithm’s convergence while maintaining its correctness as long as the heuristic is admissible.</p>
<h2 id="Antivirus-software"><a href="#Antivirus-software" class="headerlink" title="Antivirus software"></a>Antivirus software</h2><p>Antivirus software often uses heuristic rules for detecting viruses and other forms of malware. Heuristic scanning looks for code and/or behavioral patterns common to a class or family of viruses, with different sets of rules for different viruses. If a file or executing process is found to contain matching code patterns and/or to be performing that set of activities, then the scanner infers that the file is infected. The most advanced part of behavior-based heuristic scanning is that it can work against highly randomized self-modifying/mutating (polymorphic) viruses that cannot be reliably detected by simpler string scanning methods. Heuristic scanning has the potential to detect future viruses without requiring the virus to be first detected somewhere else, submitted to the virus scanner developer, analyzed, and a detection update for the scanner provided to the scanner’s users.</p>
<h2 id="启发式算法"><a href="#启发式算法" class="headerlink" title="启发式算法"></a>启发式算法</h2><p>一个基于直观或经验构造的算法，在可接受的花费（指计算时间和空间）下给出待解决组合优化问题每一个实例的一个可行解，该可行解与最优解的偏离程度一般不能被预计。现阶段，启发式算法以仿自然体算法为主，主要有蚁群算法、模拟退火法、神经网络等。</p>
<h1 id="P-Problem-vs-NP-Problem"><a href="#P-Problem-vs-NP-Problem" class="headerlink" title="P-Problem vs NP-Problem"></a>P-Problem vs NP-Problem</h1><h2 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h2><ul>
<li>时间复杂度则表示这个算法运行得到想要的解所需的计算工作量，他探讨的是当输入值接近无穷时，算法所需工作量的变化快慢程度。<h2 id="P-Problem-polynomial"><a href="#P-Problem-polynomial" class="headerlink" title="P-Problem(polynomial)"></a>P-Problem(polynomial)</h2></li>
<li>A problem is assigned to the P (polynomial time) class if there exists at least one algorithm to solve that problem, such that the number of steps of the algorithm is bounded by a polynomial in n, where n is the length of the input.</li>
<li>算法的步数可以用多项式来表示</li>
<li>P类问题：如果一个问题可以找到一个能在多项式的时间里解决它的算法</li>
</ul>
<h2 id="NP-Problem-non-deterministic-polynomial-time-不确定性多项式时间"><a href="#NP-Problem-non-deterministic-polynomial-time-不确定性多项式时间" class="headerlink" title="NP-Problem(non-deterministic polynomial time)不确定性多项式时间"></a>NP-Problem(non-deterministic polynomial time)不确定性多项式时间</h2><ul>
<li>A problem is assigned to the NP (nondeterministic polynomial time) class if it is solvable in polynomial time by a nondeterministic Turing machine.</li>
<li></li>
<li>A P-problem (whose solution time is bounded by a polynomial) is always also NP. If a problem is known to be NP, and a solution to the problem is somehow known, then demonstrating the correctness of the solution can always be reduced to a single P (polynomial time) verification. If P and NP are not equivalent, then the solution of NP-problems requires (in the worst case) an exhaustive search.</li>
<li></li>
<li>NP问题：可以在多项式的时间里验证一个解的问题/可以在多项式的时间里猜出一个解的问题</li>
<li>所有的P类问题都是NP问题。</li>
<li>不知道这个问题是不是存在多项式时间内的算法，所以叫non-deterministic不确定性，但是我们可以在多项式时间内验证并得出这个问题的一个正确解。</li>
</ul>
<h2 id="NPC-problem-non-deterministic-polynomial-complete"><a href="#NPC-problem-non-deterministic-polynomial-complete" class="headerlink" title="NPC problem(non-deterministic polynomial complete)"></a>NPC problem(non-deterministic polynomial complete)</h2><ul>
<li>如果所有NP问题都能在多项式时间内转化为最难的一个NP类问题（被约化成的问题应具有比前一个问题更复杂的时间复杂度），则称该NP问题为NPC问题</li>
</ul>
<h2 id="NP-hard-Problem"><a href="#NP-hard-Problem" class="headerlink" title="NP-hard Problem"></a>NP-hard Problem</h2><ul>
<li><p>A problem is said to be NP-hard if an algorithm for solving it can be translated into one for solving any other NP-problem. It is much easier to show that a problem is NP than to show that it is NP-hard. A problem which is both NP and NP-hard is called an NP-complete problem.</p>
</li>
<li><p>我们又叫NP难问题，他不是一个NP问题，然后所有的NPC问题都可以在多项式时间内转化为他的话，我们就叫他NPH（hard）问题。</p>
</li>
</ul>
<p>Reference:</p>
<ol>
<li><a href="https://en.wikipedia.org/wiki/Heuristic_(computer_science)" target="_blank" rel="noopener">wikipedia</a></li>
<li><a href="https://mathworld.wolfram.com/NP-Problem.html" target="_blank" rel="noopener">mathworld</a></li>
<li><a href="http://www.matrix67.com/blog/archives/105" target="_blank" rel="noopener">matrix67.com</a></li>
<li><a href="https://blog.csdn.net/databatman/article/details/49304295" target="_blank" rel="noopener">blog.csdn</a></li>
</ol>
]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
      </tags>
  </entry>
  <entry>
    <title>cloud computing notes</title>
    <url>/posts/983f472e/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Cloud Computing</tag>
      </tags>
  </entry>
  <entry>
    <title>big data notes</title>
    <url>/posts/b96255d4/</url>
    <content><![CDATA[<h1 id="CPU-vs-GPU-vs-FPGA"><a href="#CPU-vs-GPU-vs-FPGA" class="headerlink" title="CPU vs GPU vs FPGA"></a>CPU vs GPU vs FPGA</h1><p>A CPU (central processing unit) works together with a GPU (graphics processing unit) to increase the throughput of data and the number of concurrent calculations within an application.</p>
<a id="more"></a>
<p>A CPU can never be fully replaced by a GPU: a GPU complements CPU architecture by allowing repetitive calculations within an application to be run in parallel while the main program continues to run on the CPU. The CPU can be thought of as the taskmaster of the entire system, coordinating a wide range of general-purpose computing tasks, with the GPU performing a narrower range of more specialized tasks (usually mathematical). Using the power of parallelism, a GPU can complete more work in the same amount of time as compared to a CPU.</p>
<p><code>The main difference</code> between CPU and GPU architecture is that a CPU is designed to handle a wide-range of tasks quickly (as measured by CPU clock speed), but are limited in the concurrency of tasks that can be running. A GPU is designed to quickly render high-resolution images and video concurrently.</p>
<p>Because GPUs can perform parallel operations on multiple sets of data, they are also commonly used for non-graphical tasks such as machine learning and scientific computation. Designed with thousands of processor cores running simultaneously, GPUs enable massive parallelism where each core is focused on making efficient calculations.</p>
<p><img src="/images/cpu_vs_gpu.png" alt="cpu vs gpu"></p>
<p>GPU needs to use cuda, so programming on GPU is more complicated</p>
<p>GPU has better energy efficiency in design than CPU.</p>
<p>Field Programmable Gate Array (FPGA), a reconfigurable integrated circuit.</p>
<p>You can configure the FPGA to become any circuit you want to (as long as it fits on the FPGA). This is quite a bit different than the instruction-based hardware most programmers are used to, such as CPUs and GPUs. Instruction-based hardware is configured via software, whereas FPGAs are instead configured by specifying a hardware circuit.</p>
<p>Low latency<br>This is where FPGAs are much better than CPUs (or GPUs, which have to communicate via the CPU). With an FPGA it is feasible to get a latency around or below 1 microsecond, whereas with a CPU a latency smaller than 50 microseconds is already very good.</p>
<p>Reference:</p>
<ol>
<li>lectures from university of leeds</li>
<li><a href="https://www.omnisci.com/technical-glossary/cpu-vs-gpu" target="_blank" rel="noopener">omnisci</a></li>
<li><a href="https://blog.esciencecenter.nl/why-use-an-fpga-instead-of-a-cpu-or-gpu-b234cd4f309c" target="_blank" rel="noopener">esciencecenter</a></li>
</ol>
]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
      </tags>
  </entry>
  <entry>
    <title>3月7日游Chester Zoo及七种迹象，暗示你的生活正在慢慢变好</title>
    <url>/posts/537a960e/</url>
    <content><![CDATA[<h2 id="3月7日游Chester-Zoo"><a href="#3月7日游Chester-Zoo" class="headerlink" title="3月7日游Chester Zoo"></a>3月7日游Chester Zoo</h2><p>今天去了一趟Chester Zoo，去的时候和旁边的越南同学聊了一路，回的时候和日本同学聊了一路。</p>
<a id="more"></a>
<p>和越南同学聊，一直讲英语，后边问她会不会说中文，会，就穿插着用中文聊了，哈哈哈，越南同学真厉害。回的时候是日本同学，正好对日本动漫（火影忍者啥的），一些乐队（goose house），cartoon(久石让，宫崎骏)，一些风景名胜（fuji mountain，北海道），一些文化（招财猫，日本古人画像，日本天皇，doll）感兴趣，问了很多问题，还给日本同学分享了自己游玩明十三陵（一些帝王陵墓的讲究，比如有个棂星门），天安门广场（看了降旗仪式的视频），故宫里的珍妃井的故事和大戏台，聊得很开心，1个半小时很快就过去了。晚上吃过饭瞌睡，就在床上小憩了一会儿，醒来之后四周都是黑的，突然，一股难以言表的感觉浮上心头，这一天就这么过去了，时间过得好快啊。</p>
<p>最近英国疫情稍稍严峻了一些（昨天是116例了），今天第一次基本全程戴口罩，刚开始戴口罩还是很不舒服的，不过很快就习惯了。到了动物园，碰到的外国人没一个人戴口罩（此时应无视他人异样的目光），周围没人的时候就解下来，大口吸口气，哈哈哈！动物园里有很多小孩，凑我旁边问我为什么戴口罩。还好戴着口罩，化解了我脸上的尴尬。</p>
<p>刚好来利兹前去了一趟大连野生动物园，就说下最大的对比吧。这里每种动物相对少一点，人离得近一点，比如老虎区，人不是高高在上的站在围墙外面参观，而是基本水平的参观。工作人员讲解很耐心，很多景点碰到了讲解就停下来听一听。印象最深刻的就是蝙蝠馆（Fruit bat forest），蝙蝠就在身边飞啊飞，里面很臭。问了工作人员两个傻逼问题，为什么臭啊，这些蝙蝠带病毒不，哈哈哈。</p>
<p>正好看到人民日报的励志文，记录一下，当自己颓废的时候激励自己。</p>
<h2 id="重视健康和快乐"><a href="#重视健康和快乐" class="headerlink" title="重视健康和快乐"></a>重视健康和快乐</h2><ul>
<li>按时吃饭，睡觉，用心对待一日三餐，控糖戒辣，少吃高热的垃圾食品，多吃蔬菜水果，坚持锻炼，明白熬夜的坏处，规律作息，多腾出时间给自己，感受并珍惜寻常日子的小幸福。</li>
<li>健康太重要了，前段时间看了《急诊室故事》，生和死就是一瞬间的事，太脆弱了。</li>
</ul>
<h2 id="做事积极且自律"><a href="#做事积极且自律" class="headerlink" title="做事积极且自律"></a>做事积极且自律</h2><ul>
<li>自律，自律，自律</li>
<li>不要被懒惰控制，不要再荒废时光，钻研有意思的领域，不断提升自己的实力和才华</li>
<li>早早起床</li>
<li>不找借口，不畏失败，步履不停，积极且热忱地奔赴自己想要的人生。</li>
<li>这个品质可是最重要的一点啊，想要做到，就别拖，做就行了。</li>
</ul>
<h2 id="保持干净的气质"><a href="#保持干净的气质" class="headerlink" title="保持干净的气质"></a>保持干净的气质</h2><ul>
<li>干净卫生，良好的个人习惯</li>
<li>远离无意义社交</li>
</ul>
<h2 id="拥有治愈自己的能力"><a href="#拥有治愈自己的能力" class="headerlink" title="拥有治愈自己的能力"></a>拥有治愈自己的能力</h2><ul>
<li>沮丧低落的时候，和自己和解，治愈自己</li>
<li>生活真的有美食，美景和爱你的人</li>
</ul>
<h2 id="不要太在意别人的眼光"><a href="#不要太在意别人的眼光" class="headerlink" title="不要太在意别人的眼光"></a>不要太在意别人的眼光</h2><ul>
<li>忠于自己，无论是追求梦想，对待爱情，保护好那份热忱和怦然心动（单身的借口？哈哈哈）</li>
<li>按照自己的节奏踏实前行，不焦虑，不气馁，静得下心，沉得住气</li>
</ul>
<h2 id="懂得控制自己的情绪"><a href="#懂得控制自己的情绪" class="headerlink" title="懂得控制自己的情绪"></a>懂得控制自己的情绪</h2><ul>
<li>保持情绪稳定，工作的时候会遇到很多难题，不如开心地去想解决办法</li>
<li>生活里面难免遇上糟心事，别冲动。不要用别人的错误来惩罚自己。</li>
<li>善待家人</li>
<li>别说伤感情的话，别说负能量的话，少一点指责，多一点将心比心</li>
</ul>
<h2 id="心里始终装着善良和温柔"><a href="#心里始终装着善良和温柔" class="headerlink" title="心里始终装着善良和温柔"></a>心里始终装着善良和温柔</h2><ul>
<li>自觉垃圾分类，爱护小动物，不让陌生人难堪</li>
</ul>
<blockquote>
<p>愿你在平凡的一天，炙热地活，快乐地爱，不辜负时间，不错付真心，不敷衍自己。愿你成为一个温暖、善良且有力量的人，真切地为自己的人生感到骄傲。</p>
</blockquote>
<p>From 人民日报</p>
]]></content>
      <categories>
        <category>Others</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title>AI Terms</title>
    <url>/posts/d96bcfee/</url>
    <content><![CDATA[<h1 id="Bootstrapping"><a href="#Bootstrapping" class="headerlink" title="Bootstrapping"></a>Bootstrapping</h1><ol>
<li>In general</li>
</ol>
<ul>
<li>a self-starting process that is supposed to proceed without external input.</li>
</ul>
<a id="more"></a>
<ol start="2">
<li>In computer technology(usually shortened to booting)</li>
</ol>
<ul>
<li>the process of loading the basic software into the memory of a computer after power-on or general reset, especially the operating system which will then take care of loading other softwares as needed.</li>
<li>Booting is the process of starting a computer, specifically with regard to starting its software.</li>
</ul>
<ol start="3">
<li>Origins</li>
</ol>
<ul>
<li>in the early 19th-century United States (particularly in the phrase “pull oneself over a fence by one’s bootstraps”).</li>
</ul>
<ol start="4">
<li>A more practical and common interpretation</li>
</ol>
<ul>
<li>the phrase infers that when a person wants to succeed, the first thing they need to do before hard work, is to put on their boots. In this sense, the phrase makes perfect sense, as most working Americans put their shoes or boots on as the last item of clothing before they start for work.</li>
</ul>
<ol start="5">
<li>In Artificial Intelligence(AI) and Machine Learning(ML)</li>
</ol>
<ul>
<li>a technique used to iteratively improve a classifier’s performance.</li>
<li>Typically, multiple classifiers will be trained on different sets of the input data, and on prediction tasks the output of the different classifiers will be combined together.</li>
</ul>
<ol start="6">
<li>In statistics</li>
</ol>
<ul>
<li>the practice of estimating properties of an estimator (such as its variance) by measuring those properties when sampling from an approximating distribution.</li>
</ul>
<h1 id="自助（抽样）法"><a href="#自助（抽样）法" class="headerlink" title="自助（抽样）法"></a>自助（抽样）法</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul>
<li><p>自身样本重采样的方法来估计真实分布的问题</p>
</li>
<li><p>从给定训练集中有放回的均匀抽样（每当选中一个样本，它等可能地被再次选中并被再次添加到训练集中。）</p>
</li>
<li><p>对于小数据集，自助法效果很好。</p>
</li>
<li><p>最常用的一种是.632自助法，假设给定的数据集包含d个样本。该数据集有放回地抽样d次，产生d个样本的训练集。这样原数据样本中的某些样本很可能在该样本集中出现多次。没有进入该训练集的样本最终形成检验集（测试集）。 显然每个样本被选中的概率是1/d，因此未被选中的概率就是(1-1/d)，这样一个样本在训练集中没出现的概率就是d次都未被选中的概率，即(1-1/d)^d。当d趋于无穷大时，这一概率就将趋近于e^(-1)=0.368，所以留在训练集中的样本大概就占原来数据集的63.2%。</p>
</li>
<li><p>对于这部分大约36.8%的没有被采样到的数据，我们常常称之为袋外数据(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。</p>
</li>
<li><p>为了从单个样本中产生多个样本，bootstrapping作为交叉验证的一个替代方法，在原来的样本中进行替换的随机采样，从而得到新的样本。bootstrap得到的样本比交叉验证的样本重叠更多，因此它们的估计依赖性更强，被认为是小数据集进行重采样的最好方法。</p>
</li>
<li><p>例子：我要统计鱼塘里面的鱼的条数，怎么统计呢？假设鱼塘总共有鱼1000条，我是开了上帝视角的，但是你是不知道里面有多少。<br>步骤：</p>
<ol>
<li>承包鱼塘，不让别人捞鱼(规定总体分布不变)。</li>
<li>自己捞鱼，捞100条，都打上标签(构造样本)</li>
<li>把鱼放回鱼塘，休息一晚(使之混入整个鱼群，确保之后抽样随机)</li>
<li>开始捞鱼，每次捞100条，数一下，自己昨天标记的鱼有多少条，占比多少(一次重采样取分布)。</li>
<li>重复3，4步骤n次。建立分布。<br>假设一下，第一次重新捕鱼100条，发现里面有标记的鱼12条，记下为12%，放回去，再捕鱼100条，发现标记的为9条，记下9%，重复重复好多次之后，假设取置信区间95%，你会发现，每次捕鱼平均在10条左右有标记，所以，我们可以大致推测出鱼塘有1000条左右。其实是一个很简单的类似于一个比例问题。这也是因为提出者Efron给统计学顶级期刊投稿的时候被拒绝的理由–”太简单”。这也就解释了，为什么在小样本的时候，bootstrap效果较好，你这样想，如果我想统计大海里有多少鱼，你标记100000条也没用啊，因为实际数量太过庞大，你取的样本相比于太过渺小，最实际的就是，你下次再捕100000的时候，发现一条都没有标记，，，这特么就尴尬了。。。</li>
</ol>
</li>
<li><p>其核心思想和基本步骤如下：</p>
<ul>
<li>采用重抽样技术从原始样本中抽取一定数量（自己给定）的样本，此过程允许重复抽样。</li>
<li>根据抽出的样本计算给定的统计量T。</li>
<li>重复上述N次（一般大于1000），得到N个统计量T。</li>
<li>计算上述N个统计量T的样本方差，得到统计量的方差。</li>
</ul>
</li>
<li><p>通过方差的估计可以构造置信区间等</p>
</li>
<li><p>整合多个弱分类器，成为一个强大的分类器。这时候，集合分类器(Boosting, Bagging等)出现了。</p>
</li>
</ul>
<h2 id="集成学习（ensemble-learning）"><a href="#集成学习（ensemble-learning）" class="headerlink" title="集成学习（ensemble learning）"></a>集成学习（ensemble learning）</h2><ul>
<li>将若干弱分类器组合之后产生一个强分类器</li>
<li>弱分类器（weak learner）指那些分类准确率只稍好于随机猜测的分类器（error rate &lt; 50%）</li>
<li>假设各弱分类器间具有一定差异性（如不同的算法，或相同算法不同参数配置），这会导致生成的分类决策边界不同，也就是说它们在决策时会犯不同的错误。将它们结合后能得到更合理的边界，减少整体错误，实现更好的分类效果。</li>
<li>数据集较大时，可以分为不同的子集，分别进行训练，然后再合成分类器。<br>数据集过小时，可使用自助法（bootstrapping），从原样本集有放回的抽取m个子集，训练m个分类器，进行集成。</li>
<li>bagging和boosting都是集成学习（ensemble learning）领域的基本算法</li>
</ul>
<h2 id="Bagging-bootstrap-aggregation"><a href="#Bagging-bootstrap-aggregation" class="headerlink" title="Bagging(bootstrap aggregation)"></a>Bagging(bootstrap aggregation)</h2><ul>
<li><p>从训练集从进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果,至于为什么叫bootstrap aggregation，因为它抽取训练样本的时候采用的就是bootstrap的方法！</p>
</li>
<li><p>让该学习算法训练多轮，每轮的训练集由从初始的训练集中随机取出n个训练样本组成，某个训练样本在某训练集中可能出现多次或者不出现，训练之后可得到一个预测函数序列h_1,h_n，最终的预测函数H对分类问题采用投票方式，对回归问题（加权平均好点，但是没）采用简单平均方式判别。训练R个分类器f_i,分类器之间其他相同就是参数步相同。其中f_i是通过从训练集和中随机取N次样本训练得到的。对于新样本，用这个R个分类器去分类，得到最多的那个类别就是这个样本的最终类别。</p>
</li>
</ul>
<h2 id="Bagging代表算法-Random-Forest-随机森林"><a href="#Bagging代表算法-Random-Forest-随机森林" class="headerlink" title="Bagging代表算法-Random Forest(随机森林)"></a>Bagging代表算法-Random Forest(随机森林)</h2><ul>
<li><p>Bootstrap方法随机选择子样本</p>
</li>
<li><p>属性集中随机选择k个属性，每个树节点分裂时，从这随机的k个属性，选择最优的</p>
</li>
<li><p>随机森林，使用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入，就让森林中的每一棵决策树分别进行判断，看看这个样本属于那个类，然后看看哪一类被选择多，就预测为那一类。</p>
</li>
<li><p>在建立决策树的过程中，需要注意两点-采样和完全分裂。首先是两个随机采样的过程，Random Forest对输入的数据要进行 行，列的采样。</p>
</li>
<li><p>对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为n个。这样使得在训练的时候，每一棵树的输入样本都不是全部样本，使得相对不容易出现over-fitting。</p>
</li>
<li><p>然后进行列采样，从M个feature中，选择m个。</p>
</li>
<li><p>之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂，要么里面的所有样本都是指向的同一类分类。</p>
</li>
<li><p>一般的决策树都有一个重要的步骤，剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会over-fitting。</p>
</li>
<li><p>按这种算法得到的随机森林中的每一棵树都是很弱的，但是组合起来就很厉害了。可以这样比喻随机森林：每一颗决策树就是一个精通于某一个窄领域的专家，这样在随机森林中就有了很多个精通不同领域的专家，对于新的样本，可以用不同的角度看待它，最终由各个专家，投票得到结果。</p>
</li>
</ul>
<h2 id="Boosting算法代表–Adaboost-Adaptive-Boosting"><a href="#Boosting算法代表–Adaboost-Adaptive-Boosting" class="headerlink" title="Boosting算法代表–Adaboost(Adaptive Boosting)"></a>Boosting算法代表–Adaboost(Adaptive Boosting)</h2><ul>
<li><p>初始化时对每一个训练赋予相同的权重1/n（每个样本都是等概率的分布，每个分类器都会公正对待），然后用该算法对训练集训练t轮，每次训练后，对训练失败的训练列赋予较大的权重（通常是边界附近的样本），也就是让学习算法在后续的学习中集中对比较难的训练列进行训练（就是把训练分类错了的样本，再次拿出来训练，看它以后还敢出错不），整个迭代过程直到错误率足够小或达到一定次数为止，从而得到一个预测函数序列h_1,h_m，其中h_i也有一定的权重，预测效果好的预测函数权重大，反之小。最终的预测函数H对分类问题采用有权重的投票方式，对回归问题采用加权平均的方式对新样本判别。</p>
</li>
<li><p>类似bagging方法，但是训练是串行的（顺序生成），第K个分类器训练时，关注对前k-1分类器中错误，不是随机取样本，而是加大取这些分错的样本的权重。</p>
</li>
</ul>
<h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><ul>
<li>取样本方式不同：bagging采用均匀取样，而boosting根据错误率来采样，因此boosting的分类精度要由于bagging。</li>
<li>bagging的训练集选择是随机的，各轮训练集之前互相独立，而boosting的各轮训练集的选择与前面各轮的学习结果相关；bagging的各个预测函数没有权重，而boosting有权重；</li>
<li>bagging的各个函数可以并行生成，而boosting的各个预测函数只能顺序生成。</li>
<li>对于像神经网络这样极为消耗时间的算法，bagging可通过并行节省大量的时间开销。bagging和boosting都可以有效地提高分类的准确性。在大多数数据集中，boosting的准确性要比bagging高。有一些数据集中，boosting会退化-overfit。boosting思想的一种改进型adaboost方法在邮件过滤，文本分类中有很好的性能。</li>
</ul>
<h2 id="置信区间，就是一种区间估计。"><a href="#置信区间，就是一种区间估计。" class="headerlink" title="置信区间，就是一种区间估计。"></a>置信区间，就是一种区间估计。</h2><ul>
<li>例子：<ul>
<li>人口平均身高统计，这个数据肯定存在的，但只有上帝知道</li>
<li>人类统计的方法是抽样统计（不可能每个人的身高都统计到）</li>
<li>如果采用点估计，使用抽样的数据可以得到一个样本均值（平均身高），这样通过不同的抽样，就会得到不同的样本均值</li>
<li>问题来了，这样不知道哪个点估计更好</li>
<li>而采用区间估计的区间都包含了真实的期望值（真实的平均身高）</li>
<li>问题又来了，这样不知道哪个区间估计更好</li>
<li>95% 的置信区间，就是有95%的概率使估计的区间内包含期望值（构造100个这样的区间，大约会有95个区间包含期望值）</li>
<li>The 95% confidence interval defines a range of values that you can be 95% certain contains the population mean. With large samples, you know that mean with much more precision than you do with a small sample, so the confidence interval is quite narrow when computed from a large sample.<br><img src="/images/confidence_interval.jpg" alt="confidence interval"></li>
</ul>
</li>
</ul>
<h1 id="Bootstrapping-programming-in-R"><a href="#Bootstrapping-programming-in-R" class="headerlink" title="Bootstrapping programming in R"></a>Bootstrapping programming in R</h1><p>If we have n data entries, indexed as y1,…,yn and x1,…,xn, we can generate a new data set by sampling from these indices uniformly with replacement. Lets see how to do this in R using the faithful data set:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = faithful</span><br><span class="line">bootstrap_indices = sample(length(faithful), length(faithful), replace=TRUE)</span><br><span class="line">data_bootstrap = data[bootstrap_indices, ]</span><br></pre></td></tr></table></figure>
<p>data_bootstrap is now a bootstrap sample from the original data.</p>
<p>There is little to be gained from BAGGING linear regressions. This is because the sum of two linear functions is another linear function. However, for non-linear functions there is much more potential. For example,the sum of many logistic functions is not in general another logistic function. Therefore we will use logistic regression to demonstrate the power of BAGGING. We’ll define an underlying probability function that is impossible for a single logistic regression to capture properly. Then we&#39;ll show how summing over many logistic regression models trained on different sampled subsets of the data can outperform a single model.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#define a complex probability function</span></span><br><span class="line">myfunction &lt;- function(x)&#123;<span class="number">0</span>+<span class="number">1</span>*(x&gt;<span class="number">0.25</span> &amp; x &lt; <span class="number">0.75</span>)&#125;</span><br><span class="line"><span class="comment">#Generate some data</span></span><br><span class="line">X = seq(<span class="number">0</span>,<span class="number">1</span>, length.out=<span class="number">100</span>)</span><br><span class="line">P = myfunction(X)</span><br><span class="line">Y = <span class="keyword">as</span>.numeric(runif(<span class="number">100</span>)&lt;P)</span><br><span class="line"><span class="comment">#Plot the data with the actual probability</span></span><br><span class="line">plot(X, Y)</span><br><span class="line">lines(X, P)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Make a data frame and split into training and test data</span></span><br><span class="line">mydata = data.frame(Y, X)</span><br><span class="line">test_idx = sample(dim(mydata)[<span class="number">1</span>], <span class="number">20</span>)</span><br><span class="line">test_data = mydata[test_idx, ]</span><br><span class="line">train_data = mydata[-test_idx, ]</span><br><span class="line"><span class="comment">#Fit a logistic model to the training data</span></span><br><span class="line">myModel = glm(Y ~ X, data=train_data, family=binomial)</span><br><span class="line"><span class="comment">#Make a prediction for the probability of the test data being 1</span></span><br><span class="line">prediction = predict(myModel, newdata=test_data, type=<span class="string">"response"</span>)</span><br><span class="line"><span class="comment">#Evaluate the predictive log-likelihood</span></span><br><span class="line">testLL = sum(log(prediction[which(test_data$Y==<span class="number">1</span>)])) +</span><br><span class="line">  sum(log(<span class="number">1</span>-prediction[which(test_data$Y==<span class="number">0</span>)]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#BAGGING!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Make 1000 bootstrap samples</span></span><br><span class="line"><span class="comment">#Fit a logistic model to each</span></span><br><span class="line"><span class="comment">#Record prediction onto test data and aggregate</span></span><br><span class="line">bsPrediction = rep(<span class="number">0</span>, dim(test_data)[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:<span class="number">1000</span>)&#123;</span><br><span class="line">  bs_idx = sample(dim(train_data)[<span class="number">1</span>], <span class="number">20</span>, replace=TRUE)</span><br><span class="line">  bs_data = train_data[bs_idx,]</span><br><span class="line">  bsModel = glm(Y ~ X, data=bs_data, family=binomial)</span><br><span class="line">  bsPrediction = bsPrediction + predict(bsModel, newdata=test_data, type=<span class="string">"response"</span>)/<span class="number">1000</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#Get the predictive likelihood of the bagged model</span></span><br><span class="line">bstestLL = sum(log(bsPrediction[which(test_data$Y==<span class="number">1</span>)])) +</span><br><span class="line">  sum(log(<span class="number">1</span>-bsPrediction[which(test_data$Y==<span class="number">0</span>)]))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Print the results</span></span><br><span class="line">print(testLL)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(bstestLL)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot(test_data$X, prediction)</span><br><span class="line">plot(test_data$X, bsPrediction)</span><br></pre></td></tr></table></figure>
<p>In this case a single logistic regression cannot capture the fact that the probability is non-monotonic in X. By learning from different sampled subsets of the data, the BAGGED logistic regressions capture different features of the underlying relationship, in this case the rise in probability at X = 0.25 and th decrease at X = 0.75.</p>
<p>Reference:</p>
<ol>
<li>tutorial from Professor Netta Cohen</li>
<li>lectures from university of leeds</li>
<li><a href="https://en.wikipedia.org/wiki/Bootstrapping" target="_blank" rel="noopener">wikipedia</a></li>
<li><a href="https://www.jianshu.com/p/708dff71df3a" target="_blank" rel="noopener">简书</a></li>
<li><a href="https://www.jiqizhixin.com/graph/technologies/c2f1159e-e096-4c9e-8e0c-48f490dc7bbe" target="_blank" rel="noopener">机器之心</a></li>
<li><a href="https://www.zhihu.com/question/26419030" target="_blank" rel="noopener">知乎</a></li>
</ol>
]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>Artificial Intelligence</tag>
      </tags>
  </entry>
  <entry>
    <title>Data mining coursework</title>
    <url>/posts/cf5e19f/</url>
    <content><![CDATA[<blockquote>
<p>I&#39;m going to record the errors when I&#39;m doing my Data Mining coursework here</p>
</blockquote>
<a id="more"></a>
<ol>
<li><p>reindex() missing 1 required positional argument: ‘self’</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tweet_df = pd.DataFrame.reindex(index=range(df.shape[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<p>This code could help you create a DataFrame filling with ‘NaN’.<br>The new DataFrame tweet_df has one column and the same number of rows as DataFrame df.<br>The previous code is wrong and it should change to the following code:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tweet_df = pd.DataFrame().reindex(index=range(df.shape[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure></li>
<li><p>in pandas, null means numpy.nan</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> pd.isnull(df):</span><br><span class="line">  print(<span class="string">'DataFrame df is null'</span>)</span><br></pre></td></tr></table></figure>
<p>if you want to remove the null character string, you cannot merely use pd.isnull(), you should use:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop_rows</span><span class="params">(tweets)</span>:</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(tweets)):</span><br><span class="line">    <span class="keyword">if</span> tweets[i] == <span class="string">''</span> <span class="keyword">or</span> pd.isnull(tweets[i]):</span><br><span class="line">      tweets.drop([i])</span><br><span class="line">  <span class="keyword">return</span> tweets</span><br></pre></td></tr></table></figure>
<p>This code is not really good, the following code is an improvement：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop_rows</span><span class="params">(tweets)</span>:</span></span><br><span class="line">  drop_index = []</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(tweets)):</span><br><span class="line">    <span class="keyword">if</span> tweets[i] == <span class="string">''</span> <span class="keyword">or</span> pd.isnull(tweets[i]):</span><br><span class="line">      drop_index.append(i)</span><br><span class="line"></span><br><span class="line">  tweets.drop(drop_index, inplace = <span class="literal">True</span>)</span><br><span class="line">  <span class="comment"># reset_index这一步很重要，使删除了若干项的不连贯的index重新建立连续的index</span></span><br><span class="line">  tweets.reset_index(drop=<span class="literal">True</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">  <span class="keyword">return</span> tweets</span><br></pre></td></tr></table></figure></li>
<li><p>in some functions in pandas, you should be careful of inplace operator, it may bring you some misunderstanding sometimes.</p>
</li>
<li><p>Stemming(词干提取)<br>Stem (root) is the part of the word to which you add inflectional (changing/deriving) affixes such as (-ed,-ize, -s,-de,mis). So stemming a word or sentence may result in words that are not actual words. Stems are created by removing the suffixes or prefixes used with a word.</p>
<blockquote>
<p><code>Stop words</code>: <code>Stop Words</code> are words which do not contain important significance to be used in Search Queries. Usually, these words are filtered out from search queries because they return a vast amount of unnecessary information. Each programming language will give its own list of stop words to use. Mostly they are words that are commonly used in the English language such as ‘as, the, be, are’ etc.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> <span class="keyword">import</span> nltk</span><br><span class="line"> nltk.download(<span class="string">'stopwords'</span>)</span><br><span class="line"></span><br><span class="line"> <span class="keyword">from</span> wordcloud <span class="keyword">import</span> STOPWORDS</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">stop_words</span><span class="params">()</span>:</span></span><br><span class="line">   <span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line">   <span class="comment"># 179 words</span></span><br><span class="line">   stop_words_list = stopwords.words(<span class="string">'english'</span>)</span><br><span class="line">   <span class="keyword">return</span> stop_words_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 179 words</span></span><br><span class="line">nltk_list = stop_words()</span><br><span class="line"><span class="comment"># 190 words</span></span><br><span class="line">wordcloud_list = list(STOPWORDS)</span><br></pre></td></tr></table></figure>
<p><img src="/images/stopwords.png" alt="stopwords"><br><a href="https://www.nltk.org/book/ch02.html" target="_blank" rel="noopener">here</a></p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 基于Porter词干提取算法</span></span><br><span class="line"><span class="keyword">from</span> nltk.stem.porter <span class="keyword">import</span> PorterStemmer</span><br><span class="line">porter_stemmer = PorterStemmer()</span><br><span class="line">print(porter_stemmer.stem(<span class="string">'maximum'</span>))</span><br><span class="line">print(porter_stemmer.stem(<span class="string">'prints'</span>))</span><br><span class="line">print(porter_stemmer.stem(<span class="string">'printed'</span>))</span><br><span class="line">print(porter_stemmer.stem(<span class="string">'printing'</span>))</span><br><span class="line">print(porter_stemmer.stem(<span class="string">'this'</span>))</span><br><span class="line">print(porter_stemmer.stem(<span class="string">'am'</span>))</span><br><span class="line">print(porter_stemmer.stem(<span class="string">'is'</span>))</span><br><span class="line">print(porter_stemmer.stem(<span class="string">'are'</span>))</span><br><span class="line">print(porter_stemmer.stem(<span class="string">'car\'s'</span>))</span><br><span class="line">print(porter_stemmer.stem(<span class="string">'cars\''</span>))</span><br><span class="line">print(porter_stemmer.stem(<span class="string">'leaves'</span>))</span><br><span class="line">print(<span class="string">'-------------------------------------------'</span>)</span><br><span class="line"><span class="comment"># 基于Lancaster 词干提取算法</span></span><br><span class="line"><span class="keyword">from</span> nltk.stem.lancaster <span class="keyword">import</span> LancasterStemmer</span><br><span class="line">lancaster_stemmer = LancasterStemmer()</span><br><span class="line">print(lancaster_stemmer.stem(<span class="string">'maximum'</span>))</span><br><span class="line">print(lancaster_stemmer.stem(<span class="string">'prints'</span>))</span><br><span class="line">print(lancaster_stemmer.stem(<span class="string">'printed'</span>))</span><br><span class="line">print(lancaster_stemmer.stem(<span class="string">'printing'</span>))</span><br><span class="line">print(lancaster_stemmer.stem(<span class="string">'this'</span>))</span><br><span class="line">print(lancaster_stemmer.stem(<span class="string">'am'</span>))</span><br><span class="line">print(lancaster_stemmer.stem(<span class="string">'is'</span>))</span><br><span class="line">print(lancaster_stemmer.stem(<span class="string">'are'</span>))</span><br><span class="line">print(lancaster_stemmer.stem(<span class="string">'car\'s'</span>))</span><br><span class="line">print(lancaster_stemmer.stem(<span class="string">'cars\''</span>))</span><br><span class="line">print(lancaster_stemmer.stem(<span class="string">'leaves'</span>))</span><br><span class="line">print(<span class="string">'-------------------------------------------'</span>)</span><br><span class="line"><span class="comment"># 基于Snowball 词干提取算法</span></span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> SnowballStemmer</span><br><span class="line">snowball_stemmer = SnowballStemmer(<span class="string">'english'</span>)</span><br><span class="line">print(snowball_stemmer.stem(<span class="string">'maximum'</span>))</span><br><span class="line">print(snowball_stemmer.stem(<span class="string">'prints'</span>))</span><br><span class="line">print(snowball_stemmer.stem(<span class="string">'printed'</span>))</span><br><span class="line">print(snowball_stemmer.stem(<span class="string">'printing'</span>))</span><br><span class="line">print(snowball_stemmer.stem(<span class="string">'this'</span>))</span><br><span class="line">print(snowball_stemmer.stem(<span class="string">'am'</span>))</span><br><span class="line">print(snowball_stemmer.stem(<span class="string">'is'</span>))</span><br><span class="line">print(snowball_stemmer.stem(<span class="string">'are'</span>))</span><br><span class="line">print(snowball_stemmer.stem(<span class="string">'car\'s'</span>))</span><br><span class="line">print(snowball_stemmer.stem(<span class="string">'cars\''</span>))</span><br><span class="line">print(snowball_stemmer.stem(<span class="string">'leaves'</span>))</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>Lemmatisation(词性还原)<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.download(<span class="string">'wordnet'</span>)</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line">lemmatizer = WordNetLemmatizer()</span><br><span class="line">print(lemmatizer.lemmatize(<span class="string">'maximum'</span>))</span><br><span class="line">print(lemmatizer.lemmatize(<span class="string">'prints'</span>))</span><br><span class="line">print(lemmatizer.lemmatize(<span class="string">'printed'</span>))</span><br><span class="line">print(lemmatizer.lemmatize(<span class="string">'printing'</span>))</span><br><span class="line">print(lemmatizer.lemmatize(<span class="string">'this'</span>))</span><br><span class="line">print(lemmatizer.lemmatize(<span class="string">'am'</span>))</span><br><span class="line">print(lemmatizer.lemmatize(<span class="string">'is'</span>))</span><br><span class="line">print(lemmatizer.lemmatize(<span class="string">'are'</span>))</span><br><span class="line">print(lemmatizer.lemmatize(<span class="string">'car\'s'</span>))</span><br><span class="line">print(lemmatizer.lemmatize(<span class="string">'cars\''</span>))</span><br><span class="line">print(lemmatizer.lemmatize(<span class="string">'leaves'</span>))</span><br></pre></td></tr></table></figure></li>
<li>str.replace()的使用误区<br>此处使用的是pandas.Series.str.replace方法<blockquote>
<p>小tip：在路径中去除掉一些特殊意义的字符进行转义，除了可以使用’\‘之外，我们也可以使用r加在所要处理的字符外面，这样就不用专门的去处理引号之中的特殊字符了</p>
</blockquote>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_list = [<span class="string">'user'</span>, <span class="string">'url'</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(word_list)):</span><br><span class="line">  df[<span class="string">'tweet'</span>] = df[<span class="string">'tweet'</span>].str.replace(word_list[i], <span class="string">""</span>)</span><br></pre></td></tr></table></figure>
<p>本意是想替换tweets中的所有user，url，只整体替换单词，最后发现只要tweets中出现了这个组合的词都被替换了，比如一个词，原本是username，本无意替换，这里却替换成了name。<br>最后采用lambda表达式，一行搞定：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">'tweet'</span>] = df[<span class="string">'tweet'</span>].apply(<span class="keyword">lambda</span> x: <span class="string">' '</span>.join([w <span class="keyword">for</span> w <span class="keyword">in</span> x.split() <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> word_list <span class="keyword">and</span> len(w)&gt;<span class="number">3</span>]))</span><br></pre></td></tr></table></figure>
<p>意思就是把每一条tweets进行遍历（x的作用），每条tweet都根据空格分隔为一个list然后遍历（w），满足条件的留下来（for前面的w），最后每一条tweet通过’ ‘（空格）再连接起来。</p>
<ol start="7">
<li>pandas.DataFrame.merge使用<br>Merge DataFrame or named Series objects with a database-style join.<br>以数据库风格的方式合并DataFrame或Series<br>比如两个csv文件都拥有相同的列名id，现在想根据id合并为一个csv文件（类似Excel的vlookup函数）<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">outfile = pd.merge(df1, df2, how=<span class="string">'inner'</span>, left_on=<span class="string">'id'</span>,right_on=<span class="string">'id'</span>)</span><br></pre></td></tr></table></figure>
表示将df1的id列和df2的id列以inner join的方式进行合并，<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> df1 <span class="keyword">inner</span> <span class="keyword">join</span> df2</span><br><span class="line"><span class="keyword">where</span> df1.id = df2.id</span><br></pre></td></tr></table></figure>
以上代码过于啰嗦，完全可以用以下代替<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">outfile = pd.merge(df1, df2, on=<span class="string">'id'</span>)</span><br></pre></td></tr></table></figure></li>
<li>summary</li>
</ol>
<p>The method I used to create the training data and test data:</p>
<ul>
<li>get all the distinct words from different tweets(for example, get offensive words from all the offensive tweets), count and rank them.</li>
<li>In this case, I get 7 different sets of words(offensive, not offensive, targeted, nottargeted and so on).</li>
<li>Then, I choose the words that occur more than 10 times(we can modify this threshold), so I get 7 new sets of words.</li>
<li>After that, I use these 7 new sets of words to filter the original tweets respectively(for example, use new offensive words set to filter the offensive tweets, only keep words in the new offensive words set).</li>
<li>finally, I get 7 different types of tweets(offensive tweets, notoffensive tweets and so on) and combine them to 3 new tweets sets(subtask_a, subtask_b, subtask_c)</li>
</ul>
<p>Other data cleaning method I used (maybe they can be mentioned in the presentation)</p>
<ul>
<li>Only keep the characters and hash tags</li>
<li>remove the extra blanks</li>
<li>lowercase all the words</li>
<li>remove the stop words and some words that don’t make sense.(stop words are from nltk corpus and wordcloud stopwords list)</li>
<li>replace all the null subtask to NaN</li>
<li>keep the words longer than 3 words</li>
<li>word cloud presentation</li>
</ul>
<p>lemmatizer</p>
<ul>
<li>use lemmatizer for the training and test data</li>
</ul>
<p>stem</p>
<ul>
<li><p>use SnowballStemmer for the training and test data</p>
</li>
<li><p>words_count</p>
</li>
</ul>
<p>All the words(except the stop words and some words that don’t make sense)in the tweets and their counts</p>
<p>Reference:</p>
<ol>
<li><a href="https://pandas.pydata.org/pandas-docs/stable/reference/index.html" target="_blank" rel="noopener">Pandas API reference</a></li>
</ol>
]]></content>
      <categories>
        <category>Data mining</category>
      </categories>
      <tags>
        <tag>Data mining</tag>
      </tags>
  </entry>
  <entry>
    <title>Data Challenge slides</title>
    <url>/posts/70cdea1a/</url>
    <content><![CDATA[<blockquote>
<p>The slides that I use in Data Challenge.</p>
</blockquote>
<a id="more"></a>
<p><img src="/images/data-challenge-slides/1.PNG" alt="page1"><br><img src="/images/data-challenge-slides/2.PNG" alt="page2"><br><img src="/images/data-challenge-slides/3.PNG" alt="page3"><br><img src="/images/data-challenge-slides/4.PNG" alt="page4"><br><img src="/images/data-challenge-slides/5.PNG" alt="page5"><br><img src="/images/data-challenge-slides/6.PNG" alt="page6"><br><img src="/images/data-challenge-slides/7.PNG" alt="page7"><br><img src="/images/data-challenge-slides/8.PNG" alt="page8"><br><img src="/images/data-challenge-slides/9.PNG" alt="page9"><br><img src="/images/data-challenge-slides/10.PNG" alt="page10"><br><img src="/images/data-challenge-slides/11.PNG" alt="page11"><br><img src="/images/data-challenge-slides/12.PNG" alt="page12"><br><img src="/images/data-challenge-slides/13.PNG" alt="page13"><br><img src="/images/data-challenge-slides/14.PNG" alt="page14"><br><img src="/images/data-challenge-slides/15.PNG" alt="page15"><br><img src="/images/data-challenge-slides/16.PNG" alt="page16"><br><img src="/images/data-challenge-slides/17.PNG" alt="page17"></p>
<div class="note info no-icon">
            <p>In addition, we use sql programming to deal with the data and use Mice method to replace the null value in python.</p>
          </div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> impyute.imputation.cs <span class="keyword">import</span> mice</span><br><span class="line">imputer_mice = mice(data)</span><br></pre></td></tr></table></figure>
<p><img src="/images/data-challenge-slides/sql.png" alt="sql"></p>
]]></content>
      <tags>
        <tag>Data challenge</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算学习笔记之Docker(Cloud Computing Lab -- Docker)</title>
    <url>/posts/db3e7c66/</url>
    <content><![CDATA[<blockquote>
<p>Record the issues when I use Docker.</p>
</blockquote>
<a id="more"></a>
<ul>
<li>Docker在产业里的应用比VM更多</li>
<li>VM和Docker的区别在于：VM是由Hypervisor统一管理，使用Guest OS；而Docker通过Docker Engine</li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Development</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode</title>
    <url>/posts/39d23ebd/</url>
    <content><![CDATA[<p>这里分享leetcode编程练习中遇到的不错的代码。</p>
]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>Competition</tag>
      </tags>
  </entry>
  <entry>
    <title>Big Data Systems Lab Session Running Hadoop on a Single Node</title>
    <url>/posts/e8018068/</url>
    <content><![CDATA[<div class="note info">
            <p>针对第三周实验的一些简单个人理解，有错误请指出。</p>
          </div>
<a id="more"></a>
<h1 id="Initial-system-setup"><a href="#Initial-system-setup" class="headerlink" title="Initial system setup"></a>Initial system setup</h1><ol>
<li><code>whoami</code>: 显示当前用户<br>虚拟机进入时默认为root。</li>
<li><code>su hduser</code>: su命令用于切换为<code>hduser</code>用户(hadoop user为该hadoop实例下自带的用户)</li>
<li><code>cd ~</code>: 进入当前用户的根目录，如果当前是root用户，则进入<code>/root/</code>，如果当前是hduser，则进入<code>/home/hduser</code>,其中<code>/</code>代表整个虚拟机的根路径</li>
<li><code>ssh-keygen -t rsa -P &quot;&quot;</code>: <code>ssh-keygen</code>用来生成密钥对。<code>-t rsa</code>指密钥类型为rsa, <code>-P</code>提供旧密语（类似默认密码）<br>生成的密钥默认位于<code>~/.ssh/</code>目录下，有两个文件，<code>id_rsa</code>是私钥，<code>id_rsa.pub</code>是公钥（<code>.ssh</code>代表隐藏文件，基于安全目的）<br>私钥不要泄漏出去，公钥则可以随意对外发布。用公钥进行加密的数据，只能用私钥才能解密。<br>使用ssh是为了使当前虚拟机与<code>localhost</code>建立安全连接（类似两台不同主机，不同服务器之间），还能免去输入密码，提高登录速度。</li>
<li><code>cat $HOME/.ssh/id_rsa.pub &gt;&gt; $HOME/.ssh/authorized_keys</code>: 把公钥内容添加到需要免密登录的主机中，这里<code>$HOME</code>和<code>~</code>的效果是一样的<blockquote>
<p>如果想同时配置多个ssh密钥对，例如还要同时连接gitlab, github，需要手动配置config文件指定使用哪个密钥，这里留个坑，以后再补充。</p>
</blockquote>
</li>
<li><code>ssh localhost</code>: 刚才建立密钥对，就是为了连接<code>localhost</code></li>
<li><code>echo $HADOOP_HOME</code>: 这里用来显示<code>$HADOOP_HOME</code>环境变量值，指定<code>Hadoop</code>运行环境的路径<br><code>~/.bashrc</code>这里储存了当前用户下的所有环境变量，只会对当前用户有效，每次启动<code>shell</code>自动被读取</li>
<li><code>source .bashrc</code>: 运行此命令，使新建立的环境变量在当前shell中起作用，否则只能下次启动shell时激活该环境变量。<h1 id="Setting-up-Hadoop-and-HDFS"><a href="#Setting-up-Hadoop-and-HDFS" class="headerlink" title="Setting up Hadoop and HDFS"></a>Setting up Hadoop and HDFS</h1><div class="note info">
            <p>这一部分主要是配置<code>Hadoop</code>存储数据文件的路径，监听端口。<br><code>hdfsTMP</code>用来储存临时文件</p>
          </div></li>
<li><code>nano core-site.xml</code>: 同<code>vi/vim</code>作用类似，文本编辑器，编辑<code>core-site.xml</code>文件。<br>这里使用<code>nano -w core-site.xml</code>命令更好，防止自动换行，造成不必要的错误。<br>常用操作：<code>Ctrl+O</code>(保存), <code>Ctrl+X</code>(退出), <code>Ctrl+C</code>(取消操作), <code>Ctrl+W</code>，搜索, 想再次搜索相同的字符串，按<code>Alt+W</code><br><code>vi/vim core-site.xml</code>: 常用操作：<code>i</code>(进入编辑模式)，<code>ESC</code>(退出编辑模式)，<code>:wq</code>(保存并退出)，<code>:q</code>(退出不保存)，<code>:q!</code>(强制退出)</li>
<li><code>mkdir -p ~/hdfsTMP/namenode</code>: <code>-p</code>递归创建多个目录</li>
<li><code>unset HADOOP_OPTS</code>: 清除环境变量<code>$HADOOP_OPTS</code><h1 id="Formatting-HDFS"><a href="#Formatting-HDFS" class="headerlink" title="Formatting HDFS"></a>Formatting HDFS</h1><div class="note info">
            <p>创建Hadoop file system</p>
          </div></li>
<li><code>start-all.sh</code>: 没有权限时需要使用<code>chmod u+x 文件名.sh</code>来给可执行权限，不过该实验不用。<br>bash脚本执行方法：当前目录下，<code>./start-all.sh</code> 或 <code>sh start-all.sh</code><h1 id="Uploading-data-to-HDFS"><a href="#Uploading-data-to-HDFS" class="headerlink" title="Uploading data to HDFS"></a>Uploading data to HDFS</h1><h1 id="Creating-a-MapReduce-Java-application"><a href="#Creating-a-MapReduce-Java-application" class="headerlink" title="Creating a MapReduce Java application"></a>Creating a MapReduce Java application</h1>这里有个坑，<code>hadoop jar OurMapReduceJob.jar org.myorg.WordCount hdfsbook.txt output</code>跑完之后会报找不到main方法的错，原因是环境变量中的引号可能是中文的，回去检查一下。</li>
</ol>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
      </tags>
  </entry>
  <entry>
    <title>how to use gitlab and github at the same time</title>
    <url>/posts/c406e4ba/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>送给25岁的你的10个礼物</title>
    <url>/posts/2bda7ee1/</url>
    <content><![CDATA[<div class="note info">
            <p>Summary from <a href="https://www.youtube.com/watch?v=XUEhEf7Sm0c" target="_blank" rel="noopener">Youtube</a></p>
          </div>
]]></content>
      <categories>
        <category>Others</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title>为什么我不使用hexo-asset-image插件 (Why I do not use hexo-asset-image)</title>
    <url>/posts/bbd6e04f/</url>
    <content><![CDATA[<p><code>hexo-asset-image</code> 是一个图片管理插件，在博客同一级可以生成相应的文件夹，用来存放博客用到的图片。<br>但在实际使用时，发现：</p>
<ul>
<li>1.这个功能有些多余,在一定程度上造成博客的插件众多，降低访问速度。</li>
<li>2.图片和博客同时存在_post/，不如手动建一个文件夹存在images/更为合适。</li>
<li>3.插件本身因为长时间无人维护，有一些坑。具体哪些坑，以后更新…</li>
</ul>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Data mining notes &amp; weka notes</title>
    <url>/posts/c94f917b/</url>
    <content><![CDATA[<blockquote>
<p>What I learned in Data Mining.</p>
</blockquote>
<a id="more"></a>
<h1 id="Information-Extraction"><a href="#Information-Extraction" class="headerlink" title="Information Extraction"></a>Information Extraction</h1><h2 id="Information-Extraction-IE-and-Information-Retrieval-IR"><a href="#Information-Extraction-IE-and-Information-Retrieval-IR" class="headerlink" title="Information Extraction(IE) and Information Retrieval(IR)"></a>Information Extraction(IE) and Information Retrieval(IR)</h2><p>IE pulls facts and structured information from the content of large text collections (usually corpora).<br>IR pulls documents from large text collections (usually the Web) in response to specific keywords or queries.<br>With traditional query engines, getting the facts can be hard and slow.</p>
<ul>
<li>IE would return information in a structured way</li>
<li>IR would return documents containing the relevant information somewhere (if you were lucky)<br>IE returns knowledge at a much deeper level than IR.<br>When would you use IE?</li>
<li>For access to news</li>
<li>For access to scientific reports<h2 id="Named-Entity-Recognition"><a href="#Named-Entity-Recognition" class="headerlink" title="Named Entity Recognition"></a>Named Entity Recognition</h2>Identification of proper names in texts, and their classification into a set of predefined categories of interest</li>
</ul>
<h2 id="MUSE-–-MUlti-Source-Entity-Recognition"><a href="#MUSE-–-MUlti-Source-Entity-Recognition" class="headerlink" title="MUSE – MUlti-Source Entity Recognition"></a>MUSE – MUlti-Source Entity Recognition</h2><p>An IE system developed within GATE</p>
<h1 id="Language-and-Computers"><a href="#Language-and-Computers" class="headerlink" title="Language and Computers"></a>Language and Computers</h1><h2 id="Document-classification-sort-documents-into-user-defined-classes"><a href="#Document-classification-sort-documents-into-user-defined-classes" class="headerlink" title="Document classification = sort documents into user-defined classes"></a>Document classification = sort documents into user-defined classes</h2><p>classification tasks, such as sentiment analysis<br>One simple technique for identifying languages is to use <code>n-grams</code>(stretch of n tokens(i.e., letters or words)).<br>Document classification is an example of a computer science activity called machine learning, which is itself part of the subfield of artificial intelligence</p>
<p>Supervised learning: training set and test set have been labeled with desired “correct answers”.<br>Unsupervised learning: assume there are no pre-specified categories.</p>
<p>First step in classifying or clustering documents:<br>identify properties most relevant to the decision we want to make, i.e., features<br>To make a useful system we need to tell the computer two things:</p>
<ol>
<li>Exactly which features are used and exactly how to detect them</li>
</ol>
<ul>
<li>feature engineering</li>
<li>Hard to automate this step</li>
</ul>
<ol start="2">
<li>How to weight the evidence provided by the features</li>
</ol>
<ul>
<li>Often works well to use machine learning for this<br>feature engineering:</li>
<li>Kitchen sink strategy</li>
<li>Hand-crafted strategy<br>The best features may be hard to collect reliably<br>bag of words assumption<br>Imagine that we cut up a document and put the words in a bag<br>Record the odds ratio for ham to spam as 12.5/0.8.<br>Idea of Naive Bayes: count things that occur in the test set</li>
</ul>
<h2 id="authorship-attribution"><a href="#authorship-attribution" class="headerlink" title="authorship attribution"></a>authorship attribution</h2><p>Stylometry<br>Lexical style markers</p>
<h2 id="plagiarism"><a href="#plagiarism" class="headerlink" title="plagiarism"></a>plagiarism</h2><h1 id="Data-Warehousing"><a href="#Data-Warehousing" class="headerlink" title="Data Warehousing"></a>Data Warehousing</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>We have lots of data, the point of data mining is to extract the information, knowledge and particularly get the right knowledge at the right time. The idea of data Warehousing is to make data avaliable to do data mining. you need to have a static snap-shot of data to do the evaluation.</p>
<h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><ul>
<li>a standardized data store</li>
<li>a process for bringing together disparate data from throughout an organization for decision-support purposes.</li>
</ul>
<p>use different algorithms to compare results to know which algo is better to use the same data.<br>store in the database rather in one format.</p>
<p>subject-oriented: discriminating similar languages, has a particular topic<br>integrated: harvested from different sources into a single standard format(database, social media)<br>time-variant: twitter, news(data change every time, not predicting in the future)<br>non-volatile: static snap-shot, use the same dataset(pick the data at a particular time)</p>
<p>copy of transaction data, query and analysis</p>
<h2 id="Data-Mart"><a href="#Data-Mart" class="headerlink" title="Data Mart"></a>Data Mart</h2><p>smaller, more focused data warehouse - a mini-warehouse<br>reflects the business rules of a specific business unit within an enterprise.</p>
<h2 id="Generic-Architecture-of-Data"><a href="#Generic-Architecture-of-Data" class="headerlink" title="Generic Architecture of Data"></a>Generic Architecture of Data</h2><p>transaction data: base level of data- raw material for understanding customer behavior<br>operational sumamary data: summaries are for a specific time period and utilize the transaction data for that time period.<br>decision support summary data: used to help make decisions about the business.<br>database schema: defines the structure of data, not the values of the data.<br>metadata: data<br>business rules: highest level of abstraction from operational data.<br>OLAP(ONline Analytical Processing): Data representation for ease of visualization.</p>
<p>is any data-set a data warehouse?<br>sis: no<br>library catalogue: no<br>vle: no<br>text in a textbook: yes</p>
]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Data mining</tag>
      </tags>
  </entry>
  <entry>
    <title>What I did in the data challenge</title>
    <url>/posts/9affb527/</url>
    <content><![CDATA[<p>待更新…</p>
]]></content>
      <categories>
        <category>Data Analytics</category>
      </categories>
      <tags>
        <tag>competition</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch踩坑心得</title>
    <url>/posts/dadd677d/</url>
    <content><![CDATA[<blockquote>
<p>Reocrd the issues when I use pytorch.</p>
</blockquote>
<a id="more"></a>
<h1 id="momentum的用处"><a href="#momentum的用处" class="headerlink" title="momentum的用处"></a>momentum的用处</h1><p><img src="/images/pytorch/momentum.png" alt="momentum"></p>
<div class="note info">
            <p>Image from <a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener">SGD</a></p>
          </div>
<!-- more -->
<h1 id="学习笔记"><a href="#学习笔记" class="headerlink" title="学习笔记"></a>学习笔记</h1><ul>
<li>NumPy 的替代品，目的是可在GPU上使用</li>
</ul>
<p>待更新…</p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>悼念奶奶 (In memory of grandma)</title>
    <url>/posts/5e869232/</url>
    <content><![CDATA[<blockquote>
<p>奶奶居住过的老宅孤独寂寞，亦破败不堪。这正如以往年迈的奶奶，那屋檐上的杂草见证了人事间世事的冷暖。<br>The lonely house where grandma lived was secluded and dilapidated. Just like the aged grandma in the past, the weeds on the eaves have witnessed the warmth and coldness of the world.</p>
</blockquote>
<a id="more"></a>
<p>(1929-2020/2/18)<br>奶奶永远的睡着了，奶奶一路走好。</p>
<p>若不是那个电话，我不知道我可能会被瞒多久。<br>远在英国，妈妈最初只是说奶奶出了点事情，爸爸不在家，便扯开话题，聊些别的。我心里已经有了不祥的预感，这语气似乎过于委婉，又似乎夹杂着些许犹豫。<br>正值疫情严重之时，各路口都布满了层层关卡。<br>我不知道要聊些什么，聊了几句便沉默了。<br>还是沉默。<br>妈妈对着镜头看着我，我移开视线，拨弄着盘子里的食物，把薯条一根一根叉起来，慢慢的嚼。<br>妈妈说这回奶奶病得很重，似乎还在掩饰着什么。<br>姐姐也接通了，有些急，问我懂不懂妈妈的意思。<br>我已经懂了，只是不愿意把这个词说出来。<br>小的时候，每次回奶奶家，离开的时候，奶奶和爷爷都会互相搀扶着，颤颤巍巍地走到路口送我们。<br>过了几年，只有奶奶送我们。<br>现在，再也没有人了。<br>奶奶只是睡着了。<br>希望奶奶睡得安稳，睡得安详。</p>
<p><img src="/images/in-memory-of-grandma/in-memory-of-grandma.jpg" alt="in-memory-of-grandma"></p>
<div class="note info">
            <p>Image by <a href="https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3064504" target="_blank" rel="noopener">Gerd Altmann</a> from <a href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3064504" target="_blank" rel="noopener">Pixabay</a></p>
          </div>
]]></content>
      <categories>
        <category>Others</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux command</title>
    <url>/posts/ee36c540/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Development</tag>
      </tags>
  </entry>
  <entry>
    <title>武汉加油 (Stay Strong Wuhan)</title>
    <url>/posts/69e98808/</url>
    <content><![CDATA[<p><img src="/images/stay-strong-wuhan/stay-strong-wuhan.jpg" alt="Stay_Strong_Wuhan"></p>
<div class="note info">
            <p>Image from <a href="http://opinion.people.com.cn/n1/2020/0215/c1003-31588403.html" target="_blank" rel="noopener">people.cn</a></p>
          </div>
]]></content>
      <categories>
        <category>Others</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title>This is a blog without any titles</title>
    <url>/posts/58173337/</url>
    <content><![CDATA[<h2 id="A-small-goal"><a href="#A-small-goal" class="headerlink" title="A small goal"></a>A small goal</h2><p>I&#39;m going to record <code>one Linux command</code> (<del>everyday</del>) once a week since (<del>today</del>) this week.<br>Let&#39;s see how long I can insist it.</p>
]]></content>
      <categories>
        <category>Others</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title>how to ssh to linux machine in University of Leeds</title>
    <url>/posts/cf3ac2d4/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This guide will show you how to quickly and simply set up SSH so you can access the School of Computing facilities from the comfort of your own home.</p>
<p>Note: a basic familiarity with the command line will be required for this guide, as well as knowledge of how to use a terminal based text editor such as <code>vim</code> or <code>nano</code>.</p>
<a id="more"></a>

<h1 id="SSH"><a href="#SSH" class="headerlink" title="SSH"></a>SSH</h1><p><a href="http://en.wikipedia.org/wiki/Secure_Shell" target="_blank" rel="noopener">SSH</a> is a protocol which allows, among other things, remote access to a machine through a command line interface. It uses a server - client architecture. Almost all modern Unix distributions (i.e. Linux and OSX) come with <code>ssh</code> preinstalled.</p>
<h2 id="Connecting-to-University-Systems-with-ssh"><a href="#Connecting-to-University-Systems-with-ssh" class="headerlink" title="Connecting to University Systems with ssh"></a>Connecting to University Systems with <code>ssh</code></h2><p>We will be connecting to a machine at the University called <code>remote-access.leeds.ac.uk</code>, from there we will be able to connect to any of the Linux machines that you have access to.</p>
<p>To connect to the “remote-access” server using SSH type, replacing <code>&lt;your username&gt;</code> with your University username, the following:</p>
<pre><code>ssh &lt;your username&gt;@remote-access.leeds.ac.uk</code></pre><p>You will be prompted to enter your password. Once you have done so you will be at a command prompt where any commands entered will be executed on the “remote-access” server. To close this or any other <code>ssh</code> connections type</p>
<pre><code>exit</code></pre><p>or</p>
<pre><code>logout</code></pre><p>or press control-d</p>
<p>The sole purpose of the “remote-access” server is to provide an entry point onto the University network and consequently:</p>
<ol>
<li>You have almost no file storage space on the server (just under 5MB)</li>
<li>You won’t be able to access your Linux files on this machine</li>
<li>The machine doesn’t have any useful software installed.</li>
</ol>
<p>The “remote-access” server should therefore be treated as a “pass-through” or stepping stone to get onto the machine you really want to use.</p>
<p>Now you are connected to the “remote-access” server you can <code>ssh</code> into any of the Linux machines on campus (provided you are authorised to use them!). The hostnames of the machines in DEC-10 are of the format <code>comp-p30xx</code> where xx is a two digit number between and 00-59.</p>
<p>You can do this by entering, after replacing <em>xx</em> with your favourite (two digit) number:</p>
<pre><code>ssh comp-pc30xx</code></pre><p>Please be considerate with your resource usage when other people are using the machine. You can check this by running the command <code>w</code>, which will output something like the following:</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"> <span class="number">15</span>:<span class="number">55</span>:<span class="number">40</span> up <span class="number">26</span> days,  <span class="number">4</span>:<span class="number">25</span>,  <span class="number">3</span> users,  load average: <span class="number">0.33</span>, <span class="number">0.25</span>, <span class="number">0.17</span></span><br><span class="line">USER     				 TTY      FROM             <span class="symbol">LOGIN@</span>   IDLE   JCPU   PCPU WHAT</span><br><span class="line">sc15xx           :<span class="number">0</span>       :<span class="number">0</span>               <span class="number">14</span>:<span class="number">59</span>   ?xdm?   <span class="number">1</span>:<span class="number">28</span>m  <span class="number">0.17</span>s gdm-session-wor</span><br><span class="line">sc15xx           pts/<span class="number">0</span>    :<span class="number">0</span>               <span class="number">15</span>:<span class="number">53</span>    <span class="number">1</span>:<span class="number">24</span>   <span class="number">0.06</span>s  <span class="number">0.01</span>s /usr/bin/python</span><br><span class="line">&lt;your username&gt;  pts/<span class="number">1</span>    euras01hv.leeds. <span class="number">15</span>:<span class="number">55</span>    <span class="number">0.00</span>s  <span class="number">0.04</span>s  <span class="number">0.00</span>s w</span><br></pre></td></tr></table></figure>

<p>If you see any usernames in the list other than your own then you are sharing that machine and must refrain from using an excessive amount of memory or CPU time. Most of the work you will do in your 1st year won’t be intensive enough to impact other users, e.g. editing and compiling your small C programs.</p>
<h1 id="Advanced-Usage"><a href="#Advanced-Usage" class="headerlink" title="Advanced Usage"></a>Advanced Usage</h1><p>The following sections don’t offer any additional functionality, but should make it slightly quicker to log in, i.e. fewer keystrokes.</p>
<h2 id="ssh-configuration-file"><a href="#ssh-configuration-file" class="headerlink" title="ssh configuration file"></a><code>ssh</code> configuration file</h2><p>Although it is perfectly fine to connect to the remote-access server and then manually use <code>ssh</code> to get onto the machine you wish to use, there is a slightly nicer way of doing this. <code>ssh</code> uses a simple configuration text file within which you can define commonly used hosts and various settings for each of them. The configuration file is located in the <code>.ssh</code> directory within your home directory, i.e. <code>~/.ssh/</code> and is called <code>config</code>. <em>Note that this is a “hidden” directory and will only appear when you run <code>ls</code> with the <code>-a</code> option!</em></p>
<p>Open the file in your favourite text editor (mine is <code>vim</code>) and enter the following, <strong>replacing <code>&lt;your username&gt;</code> with your university username, e.g. <code>sc15xx</code></strong>:</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">Host remote-access</span><br><span class="line">	HostName remote-access.leeds.ac.uk</span><br><span class="line"><span class="built_in">	Port </span>22</span><br><span class="line"><span class="built_in">	User </span>&lt;your username&gt;</span><br></pre></td></tr></table></figure>

<p>The configuration defined a host called “remote-access” and provides the url of the server, the port to connect to (ssh default is 22) and the username of the user you wish to connect as. You can have multiple hosts defined in your config file.</p>
<p>To use this configuration, save the file as <code>~/.ssh/config</code>. Then in a terminal enter:</p>
<pre><code>ssh remote-access</code></pre><p>You will be prompted for a password. Once you have entered your password and pressed enter everything should be as it was if you had done it without using the config file.</p>
<h2 id="Setting-up-SSH-keys"><a href="#Setting-up-SSH-keys" class="headerlink" title="Setting up SSH keys"></a>Setting up SSH keys</h2><p>Typing in your password can become bothersome so it is nice to have a way to automate this. One way is to set up a RSA key pair. This uses public key cryptography to authenticate your login. <a href="https://en.wikipedia.org/wiki/RSA_(cryptosystem)" target="_blank" rel="noopener">An RSA key has two parts, a public and private key</a>. <em>The private key must be kept private</em>; guard it with your life. First we must create a set of RSA keys. To do this enter:</p>
<pre><code>ssh-keygen -t rsa</code></pre><p>you will be asked to provide a file name to save the key as. There is a default location provided in brackets so all you have to do is press enter. You will be prompted to enter a passphrase, i.e. a password. It’s up to you whether you want to use a passphrase or not (although it is recommended). Entering a passphrase does have its benefits: the security of a key still depends on the fact that it is not visible to anyone else. Should a passphrase-protected private key fall into an unauthorized users possession, they will be unable to log in to its associated accounts until they figure out the passphrase. The only downside is that the first time you want to use the key after logging on you will have to enter the passphrase. If you choose to enter a passphrase or not press <code>enter</code> and it will generate your key. The key is located in <code>~/.ssh/id_rsa</code>.</p>
<p>Now you will have to copy the public part of the key to the server. Assuming you have followed the above instructions, enter:</p>
<pre><code>ssh-copy-id remote-access</code></pre><p>You will be presented with a notice whether this has been successful. Now that you have copied your public key to the server you will be able to login without being prompted for a password.</p>
<h2 id="Automatic-SSH-proxying-through-“remote-access”-Server"><a href="#Automatic-SSH-proxying-through-“remote-access”-Server" class="headerlink" title="Automatic SSH proxying through “remote-access” Server"></a>Automatic SSH proxying through “remote-access” Server</h2><p>The gain access to any of the machines in DEC-10 from off-campus by just entering <code>ssh comp-pc30xx</code> enter following hosts into your <code>ssh</code> config file:</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Host remote-access</span><br><span class="line">        HostName remote-access.leeds.ac.uk</span><br><span class="line">       <span class="built_in"> User </span>&lt;your username&gt;</span><br><span class="line"></span><br><span class="line">Host comp-pc*</span><br><span class="line">        ProxyCommand  ssh remote-access nc %h %p</span><br></pre></td></tr></table></figure>

<p>The configuration tells ssh that for any host that starts with <code>comp-pc</code>, i.e. the machines in DEC-10, it should proxy the connection via the remote-access machine. Diagrammatically:</p>
<figure class="highlight brainfuck"><table><tr><td class="code"><pre><span class="line"><span class="literal">+</span>--<span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">+</span>                  <span class="literal">+</span>--<span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">+</span>                <span class="literal">+</span>--<span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">+</span></span><br><span class="line"><span class="comment">|</span> <span class="comment">you</span> <span class="comment">|</span>                  <span class="comment">|</span> <span class="comment">remote</span><span class="literal">-</span><span class="comment">access</span> <span class="comment">|</span>                <span class="comment">|</span> <span class="comment">comp</span><span class="literal">-</span><span class="comment">pc30xx</span>  <span class="comment">|</span></span><br><span class="line"><span class="comment"></span><span class="literal">+</span>--<span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">+</span>                  <span class="literal">+</span>--<span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">+</span>                <span class="literal">+</span>--<span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">+</span></span><br><span class="line"><span class="comment">|</span>     &lt;<span class="comment">==============</span> <span class="comment">ssh</span><span class="literal">-</span><span class="comment">over</span><span class="literal">-</span><span class="comment">netcat</span> <span class="comment">tunnel</span> <span class="comment">==============</span>&gt;             <span class="comment">|</span></span><br><span class="line"><span class="comment"></span><span class="literal">+</span>--<span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">+</span>                  <span class="literal">+</span>--<span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">+</span>                <span class="literal">+</span>--<span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">+</span></span><br></pre></td></tr></table></figure>
<p>Now you may simply enter:</p>
<pre><code>ssh comp-pc30xx</code></pre><p>where xx is a number between 00-59 and you will have a remote shell on the specified machine!</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>University of Leeds</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction</title>
    <url>/posts/ed318fdc/</url>
    <content><![CDATA[<h2 id="My-first-blog"><a href="#My-first-blog" class="headerlink" title="My first blog"></a>My first blog</h2><p>The domain name <code>kaijiadage.com</code> is derived from the Chinese spelling of my nickname which is widely used in my social media. I bought this domain name on Apirl 22nd 2016 and ran my own .net project on this website until the graduation of the undergraduate.<br>Regrettably, the server was expired after my graduation. As a result, the website cannot work.<br>When it comes to the beginning of semester 2 in my Msc study, I reborn the idea of setting up a personal blog in order to record my study, my life.</p>
]]></content>
      <categories>
        <category>Others</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
</search>
